[AdvancedOptions]
# Explicit noise handling (0: none; 1: unknown noise level; 2: user-provided noise)
uncertaintyhandling = []
# Evaluated fcn values at X0
fvals = []
# Evaluated std values at X0
S_orig = []
# Variational components for starting
kstart = 50
# High Posterior Density region (fraction of training inputs)
hpdfrac = 0.5
# High posterior points fraction for clustering
clustering_init_hpdfrac = 0.01
# Number of clusters for initializing variational posterior components, (clustering_init_hpdfrac, n_clusters): (0.01, 1) for conservative initialization, (0.8, 50) for exploratory runs
n_clusters = 1

# Number of inducing points for SGPR when starting
num_ips = 100 * D
# Increment of inducing points for SGPR when not able to fit current train set
num_ips_increment = 100
# Max retries for retraining SGPR hyperparameters
max_retries_gp_retrain = 3
# Numberf of inducing points reinitialization for one SGPR optimization (EM)
reinit_steps = 20
# Break reinitialization during SGPR optimization if not improve
break_reinit_if_not_improve = True

# How to select inducing point for the first sgpr
ipselectmethod = "dpp"
# Whether to use bounds for inducing points
no_bounds_for_inducing_points = True
# Size of training subset, for training exact gp when having too many datapoints
subsetsize = 300
# Max number of sgpr optimization
maxgpoptsN = 3

# Base step size for stochastic gradient descent
sgdstepsize = 0.005
# Use variable component means for variational posterior
variablemeans = True
# Use variable mixture weight for variational posterior
variableweights = True
# Penalty multiplier for small mixture weights
weightpenalty = 0.1
# Fraction of allowed exceptions when computing iteration stability
tolstableexcptfrac = 0.2

# Total samples for Monte Carlo approx. of the entropy
nsent = lambda K : 100 * K ** (2 / 3)
# Total samples for preliminary Monte Carlo approx. of the entropy
nsentfast = 0
# Total samples for refined Monte Carlo approx. of the entropy
nsentfine = lambda K : 2 ** 12 * K
# Samples for fast approximation of the ELBO
nselbo = lambda K : 50 * K
# Optimize ELCBO=ELBO + beta*std in variational inference
elcbo_beta = 0
# Uncertainty weight True ELCBO for computing lower bound improvement
elcboimproweight = 3
# Minimum fractional length scale
tollength = 1e-6
# Size of cache for storing fcn evaluations
cachesize = 500
# Stochastic optimizer for varational parameters
stochasticoptimizer = "adam"
# Stopping threshold for stochastic optimization
tolfunstochastic = 1e-3
# Max iterations for stochastic optimization
maxiterstochastic = 100 * (2 + D)

# Tolerance True ELBO uncertainty for stopping (if variational posterior is stable)
tolsd = 0.1
# Stopping threshold True change of variational posterior per training point
tolskl = 0.01 * np.sqrt(D)
# Required ELCBO improvement per fcn eval before termination
tolimprovement = 0.01
# Use Gaussian approximation for symmetrized KL-divergence b\w iters
klgauss = True

## Low-density noise options
# Discount observations from from extremely low-density regions
noiseshaping = True
# Threshold from max observed value to start discounting
noiseshapingthresholds = [10]
# The threshold is in number of standard deviations, not absolute value
noiseshapingthresholds_instd = True
# Proportionality factor of added noise wrt distance from threshold
noiseshapingfactors = [0.05]
# Minimum added noise
noiseshapingmin = np.sqrt(1e-3)
# Added noise at threshold
noiseshapingmed = 1
# Maximum ratio of max/min added noise
noiseshapingmaxratio = np.Inf

## Don't touch these options (these options are either not relevant for the user or are not cleaned up)
# Base observation noise magnitude (standard deviation)
noisesize = []
# Max number of consecutive repeated measurements for noisy inputs
maxrepeatedobservations = 0
# Thinning for GP hyperparameter sampling
gpsamplethin = 5
# Initial design points for GP hyperparameter training
gptrainninit = 100
# Initial design method for GP hyperparameter training
gptraininitmethod = "rand"
# Tolerance for optimization of GP hyperparameters
gptolopt = 1e-5
# Threshold True GP variance used by regulatized acquisition fcns
tolgpvar = 1e-4
# Threshold True GP variance used to stabilize sampling
tolgpvarmcmc = 1e-4
# GP mean function
gpmeanfun = "negquad"
# GP integrated mean function
gpintmeanfun = 0
# Set stochastic optimization stepsize via GP hyperparameters
gpstochasticstepsize = False
# Upper threshold True reliability index for full retraining of GP hyperparameters
gpretrainthreshold = 1
# Compute full ELCBO also at best midpoint
elcbomidpoint = True
# Multiplier to widths from previous posterior for GP sampling (Inf = do not use previous widths)
gpsamplewidths = 5
# Weight of previous trials (per trial) for running avg of GP hyperparameter covariance
hyprunweight = 0.9
# Use weighted hyperparameter posterior covariance
weightedhypcov = True
# Minimum weight for weighted hyperparameter posterior covariance
tolcovweight = 0
# Switch to covariance sampling below this threshold of stability index
covsamplethresh = 10
# Fractional tolerance for constraint violation of variational parameters
tolconloss = 0.01
# Threshold mixture component weight for pruning
tolweight = 0
# Multiplier to threshold for pruning mixture weights
pruningthresholdmultiplier = lambda K : 1 / np.sqrt(K)
# Minimum GP observation noise
tolgpnoise = np.sqrt(1e-5)
# Prior mean over GP input length scale (in plausible units)
gplengthpriormean = np.sqrt(D / 6)
# Prior std over GP input length scale (in plausible units)
gplengthpriorstd = 0.5 * np.log(1e3)
# Upper bound True GP input lengths based True plausible box (0 = ignore)
uppergplengthfactor = 0
# Stricter upper bound True GP negative quadratic mean function
gpquadraticmeanbound = True
# Tolerance True closeness to bound constraints (fraction of total range)
tolboundx = 1e-5
# Input transform for bounded variables
boundedtransform = "probit"
# Tol for stop building vp based on initial gp
buildvptol = 3
# For faster debugging purpose
fast_debugging = False
# Required stable fcn evals for termination
tolstablecount = 60
# Number of target fcn evals per iteration (Purely post-process without additional fcn evals)
funevalsperiter = 0
# Max number of iterations
maxiter = 4
# Save to pkls every checkpointiters iterations
checkpointiters = np.Inf

# Whether to re-print column headers for optimization statistics at each iteration. If None, PyVBMC tries to guess based on plotting options.
printiterationheader = None